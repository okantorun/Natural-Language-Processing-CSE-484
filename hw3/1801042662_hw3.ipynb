{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5bdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syllable import Encoder #hecelere ayirmak icin\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "import spacy  # For preprocessing\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabe61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    unique_list = []\n",
    "    for x in list1:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    dt = datetime.now()\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267d8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = str.maketrans('şğüöçığ', 'sguocig')\n",
    "encoder = Encoder(lang=\"tr\", limitby=\"vocabulary\", limit=3000)\n",
    "\n",
    "unigram = []\n",
    "bigram = []\n",
    "trigram = []\n",
    "\n",
    "text_file = open(\"dataset.txt\", encoding = \"utf8\")\n",
    "text = text_file.read()\n",
    "\n",
    "converted_text = text.lower()\n",
    "converted_text2 = converted_text.replace(\"\\n\", \" \")\n",
    "converted_text3 = converted_text2.translate(choices)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acac96d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sent_tokenize(converted_text3):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        tokens = encoder.tokenize(j)\n",
    "        for k in generate_ngrams(tokens, 1):\n",
    "            temp.append(k)\n",
    "    unigram.append(temp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187d671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('le', 0.4121391773223877), ('rin', 0.3179928660392761), ('len', 0.3016450107097626), ('me', 0.28068289160728455), ('sel', 0.2766362726688385), ('ve', 0.27142220735549927), ('den', 0.2675270438194275), ('bir', 0.25266000628471375), ('fark', 0.2348458170890808), ('tarz', 0.23392069339752197)]\n",
      "[('de', 0.37361618876457214), ('ten', 0.26883333921432495), ('ler', 0.2675270736217499), ('saf', 0.20300935208797455), ('dan', 0.20252573490142822), ('der', 0.20074397325515747), ('le', 0.19868330657482147), ('et', 0.19309118390083313), ('re', 0.18659204244613647), ('tey', 0.1839682161808014)]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "w2v_model1 = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "w2v_model1.build_vocab(unigram, progress_per=10000)\n",
    "\n",
    "w2v_model1.train(unigram, total_examples=w2v_model1.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print(w2v_model1.wv.most_similar(positive=[\"ler\"]))\n",
    "print(w2v_model1.wv.most_similar(positive=[\"den\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defa209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sent_tokenize(converted_text3):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        tokens = encoder.tokenize(j)\n",
    "        for k in generate_ngrams(tokens, 2):\n",
    "            temp.append(k)\n",
    "    bigram.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "454c6a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('le rin', 0.6098179221153259), ('le re', 0.5886204242706299), ('le riy', 0.5081042647361755), ('ri ni', 0.45505955815315247), ('ri ne', 0.4435596764087677), ('ri nin', 0.4324989318847656), ('rin den', 0.42116403579711914), ('riy le', 0.3702508807182312), ('rin de', 0.332817018032074), ('di ger', 0.29903659224510193)]\n",
      "[('la rin', 0.5864529013633728), ('ri na', 0.5447171926498413), ('la ra', 0.5361013412475586), ('rin da', 0.4905821681022644), ('la riy', 0.4713283181190491), ('rin dan', 0.4592866003513336), ('ri ni', 0.4241659939289093), ('ri nin', 0.40827760100364685), ('ma la', 0.3106870651245117), ('la ma', 0.30490338802337646)]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "w2v_model2 = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "w2v_model2.build_vocab(bigram, progress_per=10000)\n",
    "\n",
    "w2v_model2.train(bigram, total_examples=w2v_model2.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print(w2v_model2.wv.most_similar(positive=[\"le ri\"]))\n",
    "print(w2v_model2.wv.most_similar(positive=[\"la ri\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68cea49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sent_tokenize(converted_text3):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        tokens = encoder.tokenize(j)\n",
    "        for k in generate_ngrams(tokens, 3):\n",
    "            temp.append(k)\n",
    "    trigram.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e5326f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('le ri ni', 0.6848111748695374), ('le ri nin', 0.672493577003479), ('le ri dir', 0.5192487239837646), ('tu i k', 0.41442835330963135), ('il cey le', 0.4063049256801605), ('tek le ri', 0.38082873821258545), ('va di le', 0.3710438907146454), ('ve ri le', 0.368821918964386), ('le riy le', 0.3683456778526306), ('bir bir le', 0.36548933386802673)]\n",
      "[('la ri nin', 0.6661067605018616), ('la ri ni', 0.6574625372886658), ('la ri dir', 0.5426084995269775), ('bas la ri', 0.36058956384658813), ('ma la ra', 0.3168904781341553), ('ge nis let', 0.3126460909843445), ('ma la ri', 0.3072737157344818), ('ol ma la', 0.3012573719024658), ('ca ma la', 0.3008579909801483), ('ma la riy', 0.2984161376953125)]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "w2v_model3 = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model3.build_vocab(trigram, progress_per=10000)\n",
    "\n",
    "w2v_model3.train(trigram, total_examples=w2v_model3.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "print(w2v_model3.wv.most_similar(positive=[\"le ri ne\"]))\n",
    "print(w2v_model3.wv.most_similar(positive=[\"la ri na\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bef454a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'ler' and 'lar' 0.036717862\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'ler' \" +\"and 'lar'\",w2v_model1.wv.similarity('ler', 'lar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9fe4090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'le ri' and 'le rin' 0.6098179\n",
      "Cosine similarity between 'la ri' and 'la rin' 0.58645284\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'le ri' \" +\"and 'le rin'\",w2v_model2.wv.similarity('le ri', 'le rin'))\n",
    "print(\"Cosine similarity between 'la ri' \" +\"and 'la rin'\",w2v_model2.wv.similarity('la ri', 'la rin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcdc9d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'le rin den' and 'le rin de' 0.6742107\n",
      "Cosine similarity between 'la rin dan' and 'la rin da' 0.6096369\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'le rin den' \" +\"and 'le rin de'\",w2v_model3.wv.similarity('le rin den', 'le rin de'))\n",
    "print(\"Cosine similarity between 'la rin dan' \" +\"and 'la rin da'\",w2v_model3.wv.similarity('la rin dan', 'la rin da'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27decc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
